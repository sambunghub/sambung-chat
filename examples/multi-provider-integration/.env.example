# Multi-Provider Integration Example
# This example demonstrates easy switching between AI providers using environment variables

# ═══════════════════════════════════════════════════════════════════════
# PROVIDER SELECTION (Choose ONE or configure a fallback chain)
# ═══════════════════════════════════════════════════════════════════════

# Option 1: Single Provider (Recommended for most cases)
# Set AI_PROVIDER to the provider you want to use
AI_PROVIDER=openai
# AI_PROVIDER=anthropic
# AI_PROVIDER=google
# AI_PROVIDER=groq
# AI_PROVIDER=ollama

# Option 2: Fallback Chain (High Availability)
# Set multiple providers separated by commas
# The server will use the first one with a valid API key
# AI_PROVIDER=openai,anthropic,groq

# Option 3: Cost Optimization (Cheapest First)
# Order providers by cost preference
# AI_PROVIDER=groq,google,openai,anthropic

# ═══════════════════════════════════════════════════════════════════════
# OPENAI CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════

# Required for OpenAI provider
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# Optional: Model selection (default: gpt-4o-mini)
# Available models: gpt-4o-mini, gpt-4o, o1-mini, o1-preview
OPENAI_MODEL=gpt-4o-mini

# Optional: Custom base URL (for proxies or Azure deployments)
# OPENAI_BASE_URL=https://api.openai.com/v1

# Optional: Organization ID (for org-level billing)
# OPENAI_ORGANIZATION=org-your-org-id

# ═══════════════════════════════════════════════════════════════════════
# ANTHROPIC CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════

# Required for Anthropic provider
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# Optional: Model selection (default: claude-3-5-sonnet-20241022)
# Available models: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022, claude-3-opus-20240229
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Optional: Custom base URL
# ANTHROPIC_BASE_URL=https://api.anthropic.com

# ═══════════════════════════════════════════════════════════════════════
# GOOGLE CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════

# Required for Google provider (either variable works)
# Get your API key from: https://aistudio.google.com/app/apikey
GOOGLE_GENERATIVE_AI_API_KEY=your-google-api-key-here
# GOOGLE_API_KEY=your-google-api-key-here (Alternative)

# Optional: Model selection (default: gemini-2.5-flash)
# Available models: gemini-2.5-flash, gemini-2.5-pro
GOOGLE_MODEL=gemini-2.5-flash

# ═══════════════════════════════════════════════════════════════════════
# GROQ CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════

# Required for Groq provider
# Get your API key from: https://console.groq.com/keys
GROQ_API_KEY=gsk-your-groq-api-key-here

# Optional: Model selection (default: llama-3.3-70b-versatile)
# Available models: llama-3.3-70b-versatile, llama-3.1-70b-versatile, mixtral-8x7b-32768
GROQ_MODEL=llama-3.3-70b-versatile

# Optional: Custom base URL
# GROQ_BASE_URL=https://api.groq.com/openai/v1

# ═══════════════════════════════════════════════════════════════════════
# OLLAMA CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════

# Ollama runs locally, so no API key is required
# Make sure Ollama is running: ollama serve
# Get Ollama from: https://ollama.com/

# Optional: Ollama server URL (default: http://localhost:11434/v1)
# OLLAMA_BASE_URL=http://localhost:11434/v1

# Optional: Model selection (default: llama3.2)
# Available models: llama3.2, llama3.1, mistral, codellama, etc.
# Make sure to pull the model first: ollama pull llama3.2
OLLAMA_MODEL=llama3.2

# ═══════════════════════════════════════════════════════════════════════
# SERVER CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════

# Server port (default: 3001)
PORT=3001

# Environment (development or production)
NODE_ENV=development

# ═══════════════════════════════════════════════════════════════════════
# USAGE EXAMPLES
# ═══════════════════════════════════════════════════════════════════════

# Example 1: Use OpenAI (simplest)
# AI_PROVIDER=openai
# OPENAI_API_KEY=sk-...

# Example 2: Use Anthropic with Haiku (fast & cheap)
# AI_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-3-5-haiku-20241022

# Example 3: Fallback chain for high availability
# AI_PROVIDER=openai,anthropic,groq
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# GROQ_API_KEY=gsk-...

# Example 4: Cost optimization (cheapest first)
# AI_PROVIDER=groq,google,openai
# GROQ_API_KEY=gsk-...
# GOOGLE_GENERATIVE_AI_API_KEY=...
# OPENAI_API_KEY=sk-...

# Example 5: Local AI (zero cost, privacy-first)
# AI_PROVIDER=ollama
# OLLAMA_MODEL=llama3.2

# ═══════════════════════════════════════════════════════════════════════
# TESTING
# ═══════════════════════════════════════════════════════════════════════

# Test the health check:
# curl http://localhost:3001/health

# Test with a simple message:
# curl -X POST http://localhost:3001/ai \
#   -H "Content-Type: application/json" \
#   -d '{"messages":[{"role":"user","content":"Hello!"}]}'

# Run the automated test suite:
# npm test
