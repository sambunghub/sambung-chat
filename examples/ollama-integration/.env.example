# Ollama Configuration
# Ollama runs locally and doesn't require an API key

# Optional: Select a specific model (default: llama3.2)
# Models must be pulled first with: ollama pull <model-name>
# Popular models:
#   - llama3.2 (recommended, 128K context)
#   - llama3.1 (128K context)
#   - mistral (fast, efficient)
#   - codellama (for code generation)
#   - phi3 (lightweight)
#   - gemma2 (fast, lightweight)
#   - qwen2.5 (multilingual, coding)
OLLAMA_MODEL=llama3.2

# Optional: Ollama server URL (default: http://localhost:11434)
# Use this if Ollama is running on a different machine or port
# OLLAMA_BASE_URL=http://localhost:11434

# Server Configuration
PORT=3001
NODE_ENV=development
